{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniManiezzo/eeg_signal_processing/blob/main/processamento_eeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV9n-G2XWPR9"
      },
      "outputs": [],
      "source": [
        "!pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC3CasEA6Gyf"
      },
      "outputs": [],
      "source": [
        "!pip install pymatreader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqW_RJ_vUmkH"
      },
      "outputs": [],
      "source": [
        "#core class - imports the files information\n",
        "from joblib import load as jLoad\n",
        "import numpy as np\n",
        "from numpy import genfromtxt, array, savetxt, argmax\n",
        "from numpy.random import randint\n",
        "from pickle import load as pLoad\n",
        "from mne.io import read_raw_edf, read_raw_eeglab\n",
        "from os import listdir\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#sem os dois imports de classes internas ao projeto\n",
        "\n",
        "\n",
        "'''\n",
        "    Cria uma matriz EEG de dimensões t x c x l, onde 't' é o número de ensaios, 'c' é o número de canais e 'l' é o tamanho da gravação.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    directory : str\n",
        "        Nome do diretório que contêm os ensaions do sujeito que deve ser agrupado em uma matriz singular.\n",
        "        \n",
        "    fix_length : bool, optional\n",
        "        Verdadeiro se os ensaios tem tamanhos diferentes, falso caso contrário. O valor padrão é falso.\n",
        "    length : int, optional\n",
        "        Tamanho a ser usado em todos os ensaios. Isso deve ser especificado se o \"fix_length\" for True. O valor padrão é None.\n",
        "        \n",
        "    row_channels : bool, optional\n",
        "        Veradeiro se as linhas de cada ensaio corresponderem aos canais, falso caso contrário. O valor padrão é True.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    TypeError\n",
        "        Se os parâmetros ou arquivos possuem tipo não suportado.\n",
        "    RuntimeError\n",
        "        Se um erro ocorrer para prevenir o funcionamento correto.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    eeg_matrix : ndarray\n",
        "        Matriz EEG de dimensões t x c x l.\n",
        "\n",
        "    '''\n",
        "\n",
        "def create_eeg_matrix(directory, fix_length=False, length=None, row_channels=True):\n",
        "    \n",
        "    ## Validaçõe iniciais dos valores de entrada ##\n",
        "    if type(directory) != str:\n",
        "        raise TypeError(f\"'directory' deve ser do tipo 'str', tipo recebido: '{type(directory).__name__}'\")\n",
        "    if type(fix_length) != bool:\n",
        "        raise TypeError(f\"'fix_length' deve ser do tipo 'bool', tipo recebido: '{type(fix_length).__name__}'\")\n",
        "    if fix_length:\n",
        "        if (type(length) != int and type(length) != float) or (int(length) != length):\n",
        "            raise TypeError(f\"'length' deve ser do tipo 'int', tipo recebido: '{type(length).__name__}'\")\n",
        "        if length < 1:\n",
        "            raise TypeError(f\"'length' deve ser maior que '0', valor recebido: {length}\")\n",
        "        length = int(length)\n",
        "    \n",
        "    if type(row_channels) != bool:\n",
        "        raise TypeError(f\"'row_channels' deve ser do tipo 'bool', tipo recebido: '{type(row_channels).__name__}'\")\n",
        "\n",
        "\n",
        "        ## Listando o número de amostras ##\n",
        "    try:\n",
        "      files = os.listdir(f\"{gdrive_path}/{trial}\")\n",
        "      print(\"Trial atual:\" + trial)\n",
        "    except:\n",
        "        raise\n",
        "\n",
        "    eeg_matrix = []\n",
        "    trial_length = None\n",
        "    ## Validaçõe dos arquivos no diretório de cada amostra ##\n",
        "    if len(trials) == 0:\n",
        "      raise RuntimeError(\"Diretório vazio não pode ser convertido em uma matriz EEG\")\n",
        "\n",
        "    for eeg_file in files:\n",
        "      \n",
        "      if \".\" not in eeg_file:\n",
        "        raise TypeError(f\"File type of '{eeg_file}' not found\")\n",
        "\n",
        "      file_type = eeg_file.split(\".\")[-1] \n",
        "\n",
        "      if file_type == \"set\":\n",
        "      #Necessário deixar apenas os arquivos eeg dentro da pasta de amostra\n",
        "      #if file_type not in [\"sav\", \"csv\", \"dat\", \"edf\", \"set\", \"fdt\"]:\n",
        "      #  raise TypeError(f\"Tipo de arquivo '{trial}' não suprotado. TIpos suportados: sav, csv, dat, edf\")\n",
        "      \n",
        "        # Verifica o tipo do arquivo e realiza sua importação\n",
        "        try:\n",
        "       \n",
        "          raw_data = read_raw_eeglab(f\"{gdrive_path}/{trial}/{eeg_file}\")\n",
        "          data = array(raw_data.get_data())\n",
        "          print(np.shape(data))\n",
        "\n",
        "        except:\n",
        "          raise\n",
        "\n",
        "        # Verifica condição de parâmetro de entrada inicial\n",
        "        if not row_channels:\n",
        "          data = data.T\n",
        "\n",
        "        if fix_length:\n",
        "          if len(data[0]) < length:\n",
        "            raise RuntimeError(f\"Todas as amostras deve ter pelo menos {length} de tamanho\")\n",
        "          data = data[:,:length]\n",
        "\n",
        "          #teste de conteudo na variavel data\n",
        "        else:\n",
        "          if  trial_length == None:\n",
        "            trial_length = len(data[0])\n",
        "          #else:\n",
        "            #if len(data[0]) != trial_length:\n",
        "            #  raise RuntimeError(\"Todas as amostras devem ter o mesmo tamanho\")\n",
        "        \n",
        "        eeg_matrix.append(data)\n",
        "         #print(raw_data.info.get('nchan'))\n",
        "  \n",
        "    return array(eeg_matrix, order='A', dtype= float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wclRzCGC4XDE"
      },
      "outputs": [],
      "source": [
        "#time windowing class\n",
        "'''\n",
        "    Segmenta uma matriz EEG em janelas de tempo específicas\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    eeg_matrix : ndarray\n",
        "        Uma matriz EEG de dimensções t x c x l, onde 't' corresponde ao número de ensaios, 'c' ao número de canais e 'l' é o tamanho da gravação. \n",
        "    frequency : int\n",
        "        Frequência de amostragem utilziada para a gravação da matriz EEG.\n",
        "    time_windows : list\n",
        "        Janela de tempo, em segundos, na qual os ensaios foram segmentados\n",
        "    start_type : str, optional\n",
        "        Método para selecionar o início da segmentação, isso pode ser: 'begin' (segmentação começa no começo da gravação), 'random' (segmentação começa de um ponto aleatório da gravação), \n",
        "        ou 'custom' (permite a definição do momento exato para começar a segmentação). O valor padrão é 'begin'.\n",
        "    start_time : list, optional\n",
        "        Especifica o ponto, em segundos, de iníco da segmentação de cada ensaio. Pode ser especificado apenas se 'start_type' for 'custom'. O valor padrão é None.\n",
        "    channels : list, optional\n",
        "        Lista de canais para usar na segmentação, se None, então todos os canais são usados. O valor padrão é None.\n",
        "    verbose : bool, optional\n",
        "        True para receber informação durante o processo de segmentação, False caso contrário. O valor padrão é Ture.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    TypeError\n",
        "        Se os parâmetros tem tipos não suportados.\n",
        "    RuntimeError\n",
        "        Se ocorrer um erro que impede o funcionamento.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    time_segmented_matrices : list\n",
        "        Lista com as matrizes segmentadas correspondentes a cada janela de tempo.\n",
        "'''\n",
        "\n",
        "def time_windowing(eeg_matrix, frequency, time_windows, start_type=\"begin\", start_time=None, channels=None, verbose=True):\n",
        "  \n",
        "    # Validações da matriz de entrada: eeg_matrix\n",
        "    if type(eeg_matrix).__name__ != \"ndarray\" and type(eeg_matrix) != list:\n",
        "      raise TypeError(f\"'eeg_matrix' must be 'ndarray', received: '{type(eeg_matrix).__name__}'\")\n",
        "    if type(eeg_matrix) == list:\n",
        "     eeg_matrix = array(eeg_matrix)\n",
        "    if \"float\" not in eeg_matrix.dtype.name and \"int\" not in eeg_matrix.dtype.name:\n",
        "      raise TypeError(f\"All elements of 'eeg_matrix' must be numbers\")\n",
        "    if len(eeg_matrix.shape) != 3:\n",
        "       raise TypeError(f\"'eeg_matrix' must be an matrix of dimensions t x c x l, received: {eeg_matrix.shape}\")\n",
        "\n",
        "    #Validações do parâmetro de entrada: frequency (frequência de amostragem)\n",
        "    if (type(frequency) != int and type(frequency) != float) or (int(frequency) != frequency):\n",
        "        raise TypeError(f\"'frequency' must be 'int', received: '{type(frequency).__name__}'\")\n",
        "    if frequency < 1:\n",
        "        raise TypeError(f\"'frequency' must be greater than 0, received: {frequency}\")\n",
        "    frequency = int(frequency)\n",
        "\n",
        "    #Validações do parâmetro de entrada: time_windows (janela de tempo)\n",
        "    if type(time_windows).__name__ != \"ndarray\" and type(time_windows) != list:\n",
        "        raise TypeError(f\"'time_windows' must be list of numbers, received: '{type(time_windows).__name__}'\")\n",
        "    if type(time_windows) == list:\n",
        "        time_windows = array(time_windows)\n",
        "    if \"float\" not in time_windows.dtype.name and \"int\" not in time_windows.dtype.name:\n",
        "        raise TypeError(f\"All elements of 'time_windows' must be numbers\")\n",
        "    for time_window in time_windows:\n",
        "        if not (time_window > 0):\n",
        "            raise TypeError(f\"All time windows must be greater than 0, received: {time_window}\")\n",
        "\n",
        "    #Validações do parâmetro de entrada: start_type (tipo de início da janela de tempo)\n",
        "    if start_type not in [\"begin\", \"random\", \"custom\"]:\n",
        "        raise TypeError(f\"Invalid 'start_type' must be 'begin', 'random', or 'custom', received: '{start_type}'\")\n",
        "    if start_type == \"custom\":\n",
        "        if type(start_time).__name__ != \"ndarray\" and type(start_time) != list:\n",
        "            raise TypeError(f\"'start_time' must be list of numbers, received: '{type(start_time).__name__}'\")\n",
        "     #   if not (start_time > 0):\n",
        "     #       raise TypeError(f\"'start_time' must be greater than 0, received: {start_time}\")\n",
        "     #   start = int(start_time*frequency)\n",
        "\n",
        "    #Validações do parâmetro de entrada: channles (canais que serão seguimentados)\n",
        "    if type(channels) != list and channels != None:\n",
        "        raise TypeError(f\"'channels' must be list of integers, received: '{type(channels).__name__}'\")\n",
        "    if channels != None:\n",
        "        for channel in channels:\n",
        "            if type(channel) != int:\n",
        "                raise TypeError(f\"'channels' must be list of integers, received: '{type(channel).__name__}' inside the list\")\n",
        "            if channel < 0:\n",
        "                raise TypeError(f\"All channels must be greater or equal to 0, received: {channel}\")    \n",
        "\n",
        "    #Validações do parâmetro de entrada: verbose\n",
        "    if type(verbose) != bool:\n",
        "      raise TypeError(f\"'verbose' must be 'bool', received: '{type(verbose).__name__}'\")\n",
        "\n",
        "    #Validações de janela de tempo\n",
        "    recording_length = len(eeg_matrix[0,0])\n",
        "    total_recording_time = recording_length/frequency\n",
        "\n",
        "    for time in time_windows:\n",
        "      if time > total_recording_time:\n",
        "        raise RuntimeError(f\"All time windows must be less than or equal to {total_recording_time} seconds because it is the maximum recording time available in the input EEG matrix\")\n",
        "        \n",
        "\n",
        "    #Segmentação do sinal em janelas de tempo\n",
        "    time_segmented_matrices = []\n",
        "\n",
        "    #apenas uma iteração\n",
        "    for time in time_windows:\n",
        "      #if verbose: print(f\"Início time_windowing\")\n",
        "      time_length = int(time*frequency)\n",
        "\n",
        "      time_segmented_matrix = []\n",
        "\n",
        "      #apenas uma iteração\n",
        "      for trial in range(len(eeg_matrix)):\n",
        "        #print(len(eeg_matrix))\n",
        "        #print(trial)\n",
        "        \n",
        "        if(time+start_time[trial]) > total_recording_time:\n",
        "          raise RuntimeError(f\"All time windows must be less than or equal to {total_recording_time-start_time[trial]} seconds because it is the maximum recording time available in the input EEG matrix minus the specified start time\")\n",
        "        #Deve ser um int pois vai ser utilizado como index \n",
        "        start = int(start_time[trial]*frequency)\n",
        "        try:\n",
        "          if channels == None:\n",
        "              if start_type == \"begin\":\n",
        "                segmented_trial = eeg_matrix[trial][:,:time_length]\n",
        "              elif start_type == \"random\":\n",
        "                print(f\"Recording {trial} and {start_time[trial]} in segmented_trial\")\n",
        "                random_start =  randint(0, recording_length-time_length+1)\n",
        "                segmented_trial = eeg_matrix[trial][:,random_start:random_start+time_length]\n",
        "              else:\n",
        "                #Start_type = 'Custom', utiliza o start_time refente ao trial\n",
        "                segmented_trial = eeg_matrix[trial][:,start:start+time_length]\n",
        "\n",
        "          #Se os canais forem cortados de forma diferente\n",
        "          else:\n",
        "            segmented_trial = []\n",
        "            try:\n",
        "                if start_type == \"random\":        \n",
        "                  random_start =  randint(0, recording_length-time_length+1)\n",
        "                for channel in channels:\n",
        "                    if start_type == \"begin\":\n",
        "                      segmented_trial.append(eeg_matrix[trial][channel,:time_length])\n",
        "                    elif start_type == \"random\":        \n",
        "                      segmented_trial.append(eeg_matrix[trial][channel,random_start:random_start+time_length])\n",
        "                    else:\n",
        "                      segmented_trial.append(eeg_matrix[trial][channel,start:start+time_length])\n",
        "            except:\n",
        "              raise\n",
        "            segmented_trial = array(segmented_trial)\n",
        "          \n",
        "        except:\n",
        "          raise\n",
        "\n",
        "        #Registro de sucesso da segmentação\n",
        "        time_segmented_matrix.append(segmented_trial)\n",
        "      # if verbose: print(f\"Amostra {trial} segmentada corretamente\")\n",
        "\n",
        "      time_segmented_matrices.append(array(time_segmented_matrix))\n",
        "      if verbose: print(f\"Fim do método\")\n",
        "      #if verbose: print(f\"{'*'*49}\\n\")\n",
        "    \n",
        "    return time_segmented_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF4g1iKQ4CIL"
      },
      "outputs": [],
      "source": [
        "#padding e restauração do sinal (utilizado no método dwt)\n",
        "from pywt import Wavelet\n",
        "from math import floor, ceil\n",
        "from numpy import concatenate, flipud, zeros, convolve, array\n",
        "\n",
        "'''\n",
        "    Aplica um preenchimento simétrico do tamanho especificado para o sinal de entrada.\n",
        "    OBS: técnica de zero-pad anterior a aplicação de uma transformada, adicionar zeros para preencher o sinal.\n",
        "    Zero padding allows one to use a longer FFT, which will produce a longer FFT result vector.\n",
        "\n",
        "    Parametros\n",
        "    ----------\n",
        "    signal : ndarray\n",
        "        O sinal a ser preenchido.\n",
        "    size : int, opcional\n",
        "        O tamanho do preenchimento o qual corresponde ao tamanho do filtro. O valor padrão é 8. \n",
        "      \n",
        "    Retornos\n",
        "    -------\n",
        "    padded_signal : ndarray\n",
        "        Sinal preenchido.\n",
        "\n",
        "    '''\n",
        "def padding_symmetric(signal, size=1):\n",
        "\n",
        "  padded_signal = concatenate([flipud(signal[:size]), signal, flipud(signal[-size:])])\n",
        "\n",
        "  return padded_signal\n",
        "\n",
        "'''\n",
        "    Restaura o sinal para seu tamanho original usando o filtro de reconstrução.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : ndarray\n",
        "        O signal que será restaurado.\n",
        "    reconstruction_filter : list\n",
        "        O filtro de reconstrução que será utilizado para restaurar o sinal.\n",
        "    real_len : int\n",
        "        O comprimento real do sinal.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    restored_signal : ndarray\n",
        "        O sinal restaurado de comprimento especificado.\n",
        "\n",
        "    '''\n",
        "def restore_signal(signal, reconstruction_filter, real_len):\n",
        "\n",
        "  restored_signal = zeros(2 * len(signal) + 1)\n",
        "  for i in range(len(signal)):\n",
        "    restored_signal[i * 2 + 1] = signal[i]\n",
        "  #print(f'real_len = {real_len}')\n",
        "  restored_signal = convolve(restored_signal, reconstruction_filter)\n",
        "  restored_len = len(restored_signal)\n",
        "  exceed_len = (restored_len - real_len) / 2\n",
        "  restored_signal = restored_signal[int(floor(exceed_len)):(restored_len - int(ceil(exceed_len)))]\n",
        "\n",
        "  return restored_signal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0bX59fGnT5C"
      },
      "outputs": [],
      "source": [
        "#método dwt - extrai as características do sinal após o método time_windowing\n",
        "from pywt import Wavelet\n",
        "from math import floor, ceil\n",
        "'''\n",
        "    Aplica uma Transformada Wavelet Discreta no sinal.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : ndarray\n",
        "        O sinal no qual a DWT será aplicada.\n",
        "    level : int, optional\n",
        "        Os níveis de decomposição para a DWT. O padrão é 3.\n",
        "    mother_wavelet : str, optional\n",
        "        A wavelet-mãe que será usada na DWT. A padrão é \"db4\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    restored_approx_coeff : list\n",
        "        Armazena os coeficientes de aproximação.\n",
        "    restored_detail_coeff : list\n",
        "        Armazena os coeficientes detalhados.\n",
        "\n",
        "    '''\n",
        "def dwt(signal, level=3, mother_wavelet='db4'):\n",
        "  if type(signal).__name__ != 'ndarray' and type (signal) != list:\n",
        "    raise TypeError(f\"'signal' must be 'ndarray', received: '{type(signal).__name__}'\")\n",
        "  if type(signal) == list:\n",
        "        signal = array(signal)\n",
        "  if \"float\" not in signal.dtype.name and \"int\" not in signal.dtype.name:\n",
        "        raise TypeError(f\"All elements of 'signal' must be numbers\")\n",
        "  if type(level) != int:\n",
        "        raise TypeError(f\"'level' must be 'int', received: '{type(level).__name__}'\")\n",
        "  if level < 1:\n",
        "      raise TypeError(f\"'level' must be greater than 0, received: {level}\")\n",
        "  \n",
        "  mother_wavelet_matrix = ['haar', 'db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9', 'db10', 'db11', 'db12', 'db13', 'db14', 'db15',\n",
        "                            'db16', 'db17', 'db18', 'db19', 'db20', 'db21', 'db22', 'db23', 'db24', 'db25', 'db26', 'db27', 'db28', 'db29', 'db30', 'db31', 'db32', 'db33', 'db34',\n",
        "                            'db35', 'db36', 'db37', 'db38', 'sym2', 'sym3', 'sym4', 'sym5', 'sym6', 'sym7', 'sym8', 'sym9', 'sym10', 'sym11', 'sym12', 'sym13', 'sym14', 'sym15', 'sym16', 'sym17',\n",
        "                            'sym18', 'sym19', 'sym20', 'coif1', 'coif2', 'coif3', 'coif4', 'coif5', 'coif6', 'coif7', 'coif8', 'coif9', 'coif10', 'coif11', 'coif12', 'coif13', 'coif14', 'coif15', 'coif16', 'coif17',\n",
        "                            'bior1.1', 'bior1.3', 'bior1.5', 'bior2.2', 'bior2.4', 'bior2.6', 'bior2.8', 'bior3.1', 'bior3.3', 'bior3.5', 'bior3.7', 'bior3.9', 'bior4.4', 'bior5.5', 'bior6.8', 'rbio1.1', 'rbio1.3', 'rbio1.5',\n",
        "                            'rbio2.2', 'rbio2.4', 'rbio2.6', 'rbio2.8', 'rbio3.1', 'rbio3.3', 'rbio3.5', 'rbio3.7', 'rbio3.9', 'rbio4.4', 'rbio5.5', 'rbio6.8', 'dmey', 'gaus1', 'gaus2', 'gaus3', 'gaus4', 'gaus5', 'gaus6', 'gaus7',\n",
        "                            'gaus8', 'mexh', 'morl', 'cgau1', 'cgau2', 'cgau3', 'cgau4', 'cgau5', 'cgau6', 'cgau7', 'cgau8', 'shan', 'fbsp', 'cmor']\n",
        "  if mother_wavelet not in mother_wavelet_matrix:\n",
        "      raise TypeError(\"'mother_wavelet' inválida, deve ser \" + mother_wavelet_matrix + f\" , recebido: '{mother_wavelet}'\")\n",
        "\n",
        "  original_len = len(signal)\n",
        "  approx_coeff = []\n",
        "  detail_coeff = []\n",
        "  wavelet = Wavelet(mother_wavelet)\n",
        "  low_filter = wavelet.dec_lo\n",
        "  high_filter = wavelet.dec_hi\n",
        "  filter_size = len(low_filter)\n",
        "\n",
        "  try:\n",
        "    #loop conforme os níveis de decomposição estipulados\n",
        "    for _ in range(level):\n",
        "\n",
        "      padded_signal = padding_symmetric(signal, filter_size)\n",
        "\n",
        "      #aplicação dos filtros passa alta e passa baixa\n",
        "      low_pass_filtered_signal = convolve(padded_signal, low_filter)[filter_size:(2*filter_size)+len(signal)-1] \n",
        "      low_pass_filtered_signal = low_pass_filtered_signal[1:len(low_pass_filtered_signal):2]\n",
        "      high_pass_filtered_signal = convolve(padded_signal, high_filter)[filter_size:filter_size+len(signal)+filter_size-1]\n",
        "      high_pass_filtered_signal = high_pass_filtered_signal[1:len(high_pass_filtered_signal):2]\n",
        "\n",
        "      #armazenando os coeficientes detalhados e aproximados\n",
        "      approx_coeff.append(low_pass_filtered_signal)\n",
        "      detail_coeff.append(high_pass_filtered_signal)\n",
        "\n",
        "      signal = low_pass_filtered_signal\n",
        "  except:\n",
        "    raise\n",
        "\n",
        "  low_reconstruction_filter = wavelet.rec_lo\n",
        "  high_reconstruction_filter = wavelet.rec_hi\n",
        "  real_lengths = []\n",
        "\n",
        "  for i in range(level-2,-1,-1):\n",
        "    real_lengths.append(len(approx_coeff[i]))\n",
        "  \n",
        "  real_lengths.append(original_len)\n",
        "  restored_approx_coeff = []\n",
        "\n",
        "  for i in range(level):\n",
        "    restored_signal = restore_signal(approx_coeff[i], low_reconstruction_filter, real_lengths[level-1-i])\n",
        "    for j in range(i):\n",
        "      restored_signal = restore_signal(restored_signal, low_reconstruction_filter, real_lengths[level-1-j])\n",
        "    restored_approx_coeff.append(restored_signal)\n",
        "  restored_detail_coeff = []\n",
        "\n",
        "  for i in range(level):\n",
        "      restored_signal = restore_signal(detail_coeff[i], high_reconstruction_filter, real_lengths[level-1-i])\n",
        "      for j in range(i):\n",
        "          restored_signal = restore_signal(restored_signal, high_reconstruction_filter, real_lengths[level-i-j])\n",
        "      restored_detail_coeff.append(restored_signal)\n",
        "  return restored_approx_coeff, restored_detail_coeff\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRue_nDSO2xu"
      },
      "outputs": [],
      "source": [
        "#CLASSE MAIN\n",
        "from numpy.lib.twodim_base import tril_indices\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import os, os.path\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA, FastICA\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import os, os.path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#realizando o mount com o google drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "gdrive_path = '/content/gdrive/MyDrive/Colab_Notebooks/TCC2_repository/DataSet_DENS/trials'\n",
        "\n",
        "#definindo os valores de entrada time_windowing\n",
        "time_window = [int(15124.5/250)]\n",
        "frequency = 250\n",
        "start_type = 'custom'\n",
        "\n",
        "#dados inicias da dwt\n",
        "level=5\n",
        "mother_wavelet='sym6'\n",
        "\n",
        "\n",
        "#importando o tempo de início de cada estímulo\n",
        "stimulus_path = '/content/gdrive/MyDrive/Colab_Notebooks/TCC2_repository/estimulos/all_stimulus.csv'\n",
        "time_file = pd.read_csv(f'{stimulus_path}', delimiter=';')\n",
        "trial_data = array(pd.DataFrame(time_file, columns= ['trial'],dtype= str))\n",
        "time_data = array(pd.DataFrame(time_file, columns= ['time'], dtype=float))\n",
        "label_data = array(pd.DataFrame(time_file, columns= ['label'], dtype=str))\n",
        "\n",
        "\n",
        "\n",
        "#lista e ordena os ensaios no diretório no google drive\n",
        "trials = os.listdir(gdrive_path)\n",
        "trials.sort()\n",
        "print(trials)\n",
        "print(len(trials))\n",
        "\n",
        "#variável de iteração para controle dos cortes de cada amostra\n",
        "i = 0\n",
        " \n",
        "error_trials = []\n",
        "matrix_guide = []\n",
        "all_segmented_signals = []\n",
        "truncated_segmented_signals = []\n",
        "\n",
        "\n",
        "\n",
        "#loop para criar a matriz de cada sinal e segmentá-la de acordo com os eventos\n",
        "for trial in trials:\n",
        "  eeg_matrix = create_eeg_matrix(gdrive_path)\n",
        "  print(50*\"*\")\n",
        "\n",
        "  segmented_full = []\n",
        "  #loop para segmentar os eventos\n",
        "  for trial_value in trial_data:\n",
        "\n",
        "    #condição para verificar quantos eventos cada ensaio poussi no arquivo EEG\n",
        "    if int(trial_value) == int(trial.split('t')[-1]):\n",
        "      time_stimulus = time_data[i]\n",
        "      label_stimulus = label_data[i]\n",
        "      i+=1\n",
        "      print(\"Ensaio\" + (trial.split('t')[-1]) + \": \")\n",
        "      print(time_stimulus)\n",
        "\n",
        "      recording_length = len(eeg_matrix[0,0])\n",
        "      total_recording_time = recording_length/frequency\n",
        "      \n",
        "      #referência de ensaios com erro\n",
        "      if(total_recording_time < time_stimulus+time_window):\n",
        "        error_trial = (int(trial_value), str(label_stimulus) ,float(time_stimulus))\n",
        "        error_trials.append(error_trial)\n",
        "                        \n",
        "      else:\n",
        "        #armazena os eventos segmentados em ordem\n",
        "        segmented_matrix = time_windowing(eeg_matrix, frequency, time_window, start_type, time_stimulus)\n",
        "        guide = (int(trial_value), str(label_stimulus) ,float(time_stimulus))\n",
        "        matrix_guide.append(guide)\n",
        "        segmented_full.append(segmented_matrix)\n",
        "  \n",
        "  #armazena os grupos de eventos segmentados\n",
        "  all_segmented_signals.append(segmented_full)\n",
        "\n",
        "#salva matriz com sinais já segmentados no drive\n",
        "np.save(\"/content/gdrive/MyDrive/Colab_Notebooks/TCC2_repository/resultados/all_segmented_signals.npy\", all_segmented_signals)\n",
        "\n",
        "#log de sucesso, retornando o formato das matrizes resultantes\n",
        "print(f\"Junção dos sinais full: {np.shape(all_segmented_signals)}\")\n",
        "print(\"Sucesso!\")\n",
        "print(f\"Erro trials: {np.shape(error_trials)}\")\n",
        "print(f\"Matrix_guide: {np.shape(matrix_guide)}\")\n",
        "\n",
        "#gerando arquivo de guia para os eventos que foram armazenados na matriz e seus respectivos tempos de início.\n",
        "df = pd.DataFrame({'Trial': matrix_guide[0]})\n",
        "error_matrix = np.matrix(error_trials)\n",
        "print(matrix_guide)\n",
        "pd.DataFrame(matrix_guide).to_csv(\"/content/gdrive/MyDrive/Colab_Notebooks/TCC2_repository/resultados/matrix_guide.csv\")\n",
        "\n",
        "\n",
        "normalized_approx = []\n",
        "normalized_approx_coeff = []\n",
        "normalized_approx_coeff2 = []\n",
        "\n",
        "normalized_detail = []\n",
        "normalized_detail3 = []\n",
        "normalized_detail4 = []\n",
        "\n",
        "stimulus_approx_coeff_matrix = []\n",
        "stimulus_detail_coeff_matrix = []\n",
        "\n",
        "trails_coeff = []\n",
        "\n",
        "n=0\n",
        "m=0\n",
        "i=0\n",
        "j=0\n",
        "k=0\n",
        "\n",
        "#iniciando método de normalização\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "#iniciando PCA\n",
        "pca = PCA()\n",
        "\n",
        "print(f\"Junção dos sinais full *carregados*: {np.shape(all_segmented_signals)}\")\n",
        "print(f\"i = {len(all_segmented_signals)}\")\n",
        "print(f'j = {len(all_segmented_signals[i])}')\n",
        "print(f'k= {len(all_segmented_signals[i][j][0])}')\n",
        "\n",
        "\n",
        "#loops aninhados para submeter todos os elementos da matriz à extração de características\n",
        "for i in range(len(all_segmented_signals)):\n",
        "  events_coeff = []\n",
        "  for j in range(len(all_segmented_signals[i])):\n",
        "    final_normalized_matrix = []\n",
        "    for k in range(len(all_segmented_signals[i][j][0])):\n",
        "      coeff_matrix = []\n",
        "      coeff_matrix = []\n",
        "      coeff_detail = []\n",
        "      coeff_approx = []\n",
        "      coeff_detail3 = []\n",
        "      coeff_detail4 = []\n",
        "      noarmalized_matrix = []\n",
        "\n",
        "      signal = all_segmented_signals[i][j][0][k]\n",
        "\n",
        "      #realizando a extração de características\n",
        "      approx_coeff, detail_coeff = dwt(signal, level, mother_wavelet)\n",
        "\n",
        "      #separando coeficientes de tamanhos diferentes\n",
        "      coeff_matrix.append(detail_coeff[0])\n",
        "      coeff_matrix.append(detail_coeff[1])\n",
        "      coeff_detail.append(detail_coeff[2])\n",
        "      coeff_detail3.append(detail_coeff[3])\n",
        "      coeff_detail4.append(detail_coeff[4])\n",
        "      coeff_approx.append(approx_coeff[4])\n",
        "\n",
        "      \n",
        "      #normalização e armazenamento dos coeficientes\n",
        "      normalized_matrix = scaler.fit_transform(np.reshape(coeff_matrix, (-1,1)))\n",
        "      normalized_matrix = np.reshape(normalized_matrix, (1,-1))\n",
        "\n",
        "      normalized_detail = scaler.fit_transform(np.reshape(coeff_detail, (-1,1)))\n",
        "      normalized_detail = np.reshape(normalized_detail, (1,-1))\n",
        "\n",
        "      normalized_detail3 = scaler.fit_transform(np.reshape(coeff_detail3, (-1,1)))\n",
        "      normalized_detail3 = np.reshape(normalized_detail3, (1,-1))\n",
        "\n",
        "      normalized_detail4 = scaler.fit_transform(np.reshape(coeff_detail4, (-1,1)))\n",
        "      normalized_detail4 = np.reshape(normalized_detail4, (1,-1))\n",
        "\n",
        "      normalized_approx = scaler.fit_transform(np.reshape(coeff_approx, (-1,1)))\n",
        "      normalized_approx = np.reshape(normalized_approx, (1,-1))  \n",
        "      \n",
        "      #agrupando todos os coeficientes (D1, D2, D3, D4, D5 E A5)\n",
        "      normalized_matrix = np.append(normalized_matrix, normalized_detail)\n",
        "      normalized_matrix = np.append(normalized_matrix, normalized_detail3)\n",
        "      normalized_matrix = np.append(normalized_matrix, normalized_detail4)\n",
        "      normalized_matrix = np.append(normalized_matrix, normalized_approx)\n",
        "\n",
        "      \n",
        "\n",
        "      #armazena coeficientes\n",
        "      final_normalized_matrix.append(normalized_matrix)\n",
        "\n",
        "    #laço j, seleção de características\n",
        "    select_coeff = pca.fit_transform(final_normalized_matrix) \n",
        "    events_coeff.append(select_coeff)\n",
        "    \n",
        "\n",
        "  #laço i\n",
        "  trails_coeff.append(events_coeff)\n",
        "\n",
        "  #logs de execução\n",
        "  print(i)\n",
        "  print(f'Matrix final = {np.shape(trails_coeff)}')\n",
        "  print(f'Primeira linha = {np.shape(trails_coeff[0])}') \n",
        "\n",
        "\n",
        "\n",
        "#log de resultados da extração de características\n",
        "print(f'Método invocado: {n} vezes')\n",
        "print(f'Número de blocos de 132 canais: {m}')\n",
        "print(f'Número de trials: {i}' )\n",
        "\n",
        "#armazenando a matriz com as características extraídas\n",
        "np.save(\"/content/gdrive/MyDrive/Colab_Notebooks/TCC2_repository/resultados/trial_approx_coeff_matrix.npy\",trails_coeff)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#comparação entre coeficientes antes e depois da normalização\n",
        "np.set_printoptions(threshold=np.sys.maxsize)\n",
        "print(f'Coeficientes D4: {detail_coeff[4]} ')\n",
        "print(f'Coeficientes D4 normalizados: {normalized_detail4}')\n"
      ],
      "metadata": {
        "id": "n507oHKsGlW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Geração de gráfico de distribuição de informação do PCA\n",
        "per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
        "print(per_var)\n",
        "labels = ['PC' + str(x) for x in range(1, 11)]\n",
        "\n",
        "plt.bar(x=range(1, 11), height=per_var[0:10], tick_label=labels)\n",
        "plt.rcParams['xtick.labelsize'] = 16\n",
        "plt.rcParams['ytick.labelsize'] = 16\n",
        "plt.ylabel('Porcentagem de Variância Percentual', fontsize = 18)\n",
        "plt.xlabel('Valores Singulares', fontsize = 18)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OoejM_20YIKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#geração de gráfico de decomposição\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "all_segmented_signals = np.load('/content/gdrive/MyDrive/Colab_Notebooks/TCC2_repository/resultados/all_segmented_signals.npy', allow_pickle=True)\n",
        "approx_coeff, detail_coeff = dwt(all_segmented_signals[3][0][0][120], 5, 'sym6')\n",
        "plt.rcParams['figure.figsize'] = [20,10]\n",
        "plt.rcParams['xtick.labelsize'] = 18\n",
        "plt.rcParams['ytick.labelsize'] = 18\n",
        "plt.subplot(611)\n",
        "plt.plot(range(len(detail_coeff[0])),detail_coeff[0], 'b')\n",
        "plt.subplot(612)\n",
        "plt.plot(range(len(detail_coeff[1])),detail_coeff[1], 'g')\n",
        "plt.subplot(613)\n",
        "plt.plot(range(len(detail_coeff[2])),detail_coeff[2], 'c')\n",
        "plt.subplot(614)\n",
        "plt.plot(range(len(detail_coeff[3])),detail_coeff[3], 'r')\n",
        "plt.subplot(615)\n",
        "plt.plot(range(len(detail_coeff[4])),detail_coeff[4], 'm')\n",
        "plt.subplot(616)\n",
        "plt.plot(range(len(approx_coeff[4])),approx_coeff[4], 'k')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HAZawkADT5-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificação de ensaios sem estímulos\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "all_segmented_signals = np.load('/content/gdrive/MyDrive/Colab_Notebooks/TCC2_repository/resultados/all_segmented_signals.npy', allow_pickle=True)\n",
        "for i in range(len(all_segmented_signals)):\n",
        "  print(f'{i} Tem conteúdo vazio: {all_segmented_signals[i]==[]}')"
      ],
      "metadata": {
        "id": "aC2ppI5qv0Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Geração de gráfico para comparação de estímulos sem tratamento nos sianis\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "all_segmented_signals = np.load('/content/gdrive/MyDrive/Colab_Notebooks/TCC2_repository/resultados/all_segmented_signals.npy', allow_pickle=True)\n",
        "plt.subplot(211)\n",
        "plt.plot(range(len(all_segmented_signals[3][0][0][0])),all_segmented_signals[3][0][0][120], 'k')\n",
        "plt.subplot(212)\n",
        "plt.plot(range(len(all_segmented_signals[3][1][0][120])),all_segmented_signals[3][1][0][120], 'k')"
      ],
      "metadata": {
        "id": "pfgI-4gSd82I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Manipulação dos dados obtidos\n",
        "from random import seed\n",
        "from sklearn.svm import SVC\n",
        "from numpy.lib.twodim_base import tril_indices\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import os, os.path\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA, FastICA\n",
        "from google.colab import drive\n",
        "import os, os.path\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "event_test_X = []\n",
        "event_X = []\n",
        "X = []\n",
        "y = []\n",
        "y_name = []\n",
        "y_events = []\n",
        "\n",
        "#importando dados de etapas anteriores\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "raw_X = np.load('/content/gdrive/MyDrive/Colab_Notebooks/TCC2_repository/resultados/trial_approx_coeff_matrix.npy', allow_pickle=True)\n",
        "events_classification = pd.read_csv('/content/gdrive/MyDrive/Colab_Notebooks/TCC2_repository/estimulos/stimulus_classification_new.csv')\n",
        "matrixguide = pd.read_csv('/content/gdrive/MyDrive/Colab_Notebooks/TCC2_repository/resultados/matrix_guide.csv')\n",
        "classificacao = pd.read_csv('/content/gdrive/MyDrive/Colab_Notebooks/TCC2_repository/estimulos/classificacao.csv')\n",
        "\n",
        "\n",
        "events = array(pd.DataFrame(matrixguide, columns= ['1'],dtype= str))\n",
        "\n",
        "#adequando o tamanho da matriz de resultados\n",
        "for i in range(len(events)):\n",
        "  y_events.append(events[i][0])\n",
        "\n",
        "\n",
        "print(y_events)\n",
        "print(f'events shape = {np.shape(events)}')\n",
        "\n",
        "#label of dots\n",
        "stimulus_name = array(pd.DataFrame(events_classification, columns= ['stimuliName'],dtype= str))\n",
        "\n",
        "\n",
        "#valores de escala valência, excitação e dominância\n",
        "valence = array(pd.DataFrame(events_classification, columns= ['valence'],dtype= float))\n",
        "arousal = array(pd.DataFrame(events_classification, columns= ['arousal'],dtype= float ))\n",
        "dominance = array(pd.DataFrame(events_classification, columns= ['dominance'],dtype= float ))\n",
        "label = array(pd.DataFrame(classificacao, columns= ['label'], dtype= str))\n",
        "print(pd.DataFrame(classificacao))\n",
        "\n",
        "#loop para agrupar os eventos sem os ensaios\n",
        "for i in range(len(raw_X)):\n",
        "  for j in range(len(raw_X[i])):\n",
        "      event_X.append(raw_X[i][j])\n",
        "\n",
        "print(f'Event_X shape {np.shape(event_X)}')\n",
        "\n",
        "#loop para popular X\n",
        "for i in range(len(event_X)):\n",
        "  z = []\n",
        "  for j in range(len(event_X[i])):\n",
        "    for k in range(len(event_X[i][j])):\n",
        "      z.append(event_X[i][j][k])\n",
        "  X.append(z)\n",
        "\n",
        "print(np.ndim(valence))\n",
        "print(np.shape(valence))\n",
        "\n",
        "y_valence = []\n",
        "y_arousal = []\n",
        "y_dominance = []\n",
        "#loop para popular y\n",
        "for i in range(len(valence)):\n",
        "  v = []\n",
        "  a = []\n",
        "  d = []\n",
        "  v.append(valence[i][0])\n",
        "  a.append(arousal[i][0])\n",
        "  d.append(dominance[i][0])\n",
        "  y_valence.append(v)\n",
        "  y_arousal.append(a)\n",
        "  y_dominance.append(d)\n",
        "\n",
        "\n",
        "label_emotion = []\n",
        "\n",
        "#ponto médio de cada escala\n",
        "#valência\n",
        "i = 5\n",
        "#excitação\n",
        "j = 5.1999999\n",
        "#dominância\n",
        "h = 4.0\n",
        "\n",
        "#determinando as emoções com relação ao PAD\n",
        "print(valence[0][0])\n",
        "for k in range(len(valence)):\n",
        "  if y_valence[k][0]>=i and y_arousal[k][0]>=j:\n",
        "    if y_dominance[k][0]>=h:\n",
        "      label_emotion.append(\"ALEGRE\")\n",
        "    else: \n",
        "      label_emotion.append(\"SURPRESO\")\n",
        "  elif y_valence[k][0]>=i and y_arousal[k][0]<j:\n",
        "    if y_dominance[k][0]>=h:\n",
        "      label_emotion.append(\"SATISFEITO\")\n",
        "    else:  \n",
        "      label_emotion.append(\"PROTEGIDO\")\n",
        "  elif y_valence[k][0]<i and y_arousal[k][0]>=j:\n",
        "    if y_dominance[k][0]>=h:\n",
        "      label_emotion.append(\"DESPREOCUPADO\")\n",
        "    else:\n",
        "      label_emotion.append(\"TRISTE\")\n",
        "  elif y_valence[k][0]<i and y_arousal[k][0]<j:\n",
        "    if y_dominance[k][0]>=h:\n",
        "      label_emotion.append(\"BRAVO\")\n",
        "    else:  \n",
        "      label_emotion.append(\"MEDO\")\n",
        "print(f\"i = {i} and j = {j}\")\n",
        "print(label_emotion)\n",
        "\n",
        "#Dividindo os dados para teste e treino (proporção 80% treino e 20% teste)\n",
        "treino_x, teste_x, treino_y, teste_y = train_test_split(X, label_emotion, \n",
        "                                                    test_size = 0.20, random_state=3)\n",
        "\n",
        "print(f'treino_x shape = {np.shape(treino_x)}')\n",
        "print(f'treino_y shape = {np.shape(treino_y)}')\n",
        "print(f'teste_x shape = {np.shape(teste_x)}')\n",
        "print(f'teste_y shape = {np.shape(teste_y)} ')"
      ],
      "metadata": {
        "id": "loOCtp7hETbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loop para descobrir melhor ponto intermediário\n",
        "winner = []\n",
        "accuracy_array = [0]\n",
        "for i in np.arange(4.0,5.5,0.1):\n",
        "  for j in np.arange(4.0,5.5,0.1):\n",
        "    for h in np.arange(4.0,5.5,0.1):\n",
        "      label_testing = []\n",
        "      for k in range(len(valence)):\n",
        "        if y_valence[k]>=i and y_arousal[k]>=j:\n",
        "          if y_dominance[k]>=h:\n",
        "            label_testing.append(\"ALEGRE\")\n",
        "          else: \n",
        "            label_testing.append(\"SURPRESO\")\n",
        "        elif y_valence[k]>=i and y_arousal[k]<j:\n",
        "          if y_dominance[k]>=h:\n",
        "            label_testing.append(\"SATISFEITO\")\n",
        "          else:  \n",
        "            label_testing.append(\"PROTEGIDO\")\n",
        "        elif y_valence[k]<i and y_arousal[k]>=j:\n",
        "          if y_dominance[k]>=h:\n",
        "            label_testing.append(\"DESPREOCUPADO\")\n",
        "          else:\n",
        "            label_testing.append(\"TRISTE\")\n",
        "        elif y_valence[k]<i and y_arousal[k]<j:\n",
        "          if y_dominance[k]>=h:\n",
        "            label_testing.append(\"BRAVO\")\n",
        "          else:  \n",
        "            label_testing.append(\"MEDO\")\n",
        "      print(f\"i = {i} and j = {j} and {h}\")\n",
        "      treino_x, teste_x, treino_y, teste_y = train_test_split(X, label_testing, \n",
        "                                                          test_size = 0.20, random_state=3)\n",
        "\n",
        "      model = SVC()\n",
        "      neigh = KNeighborsClassifier()\n",
        "\n",
        "      #seting the training variables\n",
        "      model.fit(treino_x,treino_y)\n",
        "      prediction = model.predict(teste_x)\n",
        "\n",
        "      #real prediction\n",
        "      prediction.view()\n",
        "      accuracy = accuracy_score(teste_y, prediction) * 100\n",
        "\n",
        "\n",
        "      print(accuracy)\n",
        "      print(max(accuracy_array))\n",
        "      if(accuracy> max(accuracy_array)):\n",
        "        print(f'Winner accuracy is: {accuracy}, with the medium valance value of {i}, arousal {j} and dominance {h}')\n",
        "        winner = [accuracy, i, j]\n",
        "        dummy_stratified = DummyClassifier()\n",
        "        dummy_stratified.fit(treino_x, treino_y)\n",
        "        previsoes = dummy_stratified.predict(teste_x)\n",
        "        accuracy_dummy = accuracy_score(teste_y, previsoes) * 100\n",
        "        print(f'Dummy classifier = {accuracy_dummy}')   \n",
        "        results_svm = pd.DataFrame({'Prediction': prediction, 'Real value': teste_y})\n",
        "        print(results_svm)\n",
        "        winner_label = label_testing\n",
        "      accuracy_array.append(accuracy)\n",
        "print(winner)"
      ],
      "metadata": {
        "id": "pJQtMOuFJTQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#knn e svm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "print(f'X shape: {np.shape(X)}')\n",
        "print(f'y_name shape: {np.shape(y_name)}')\n",
        "print(f'y shape: {np.shape(y)}')\n",
        "#print(f'y {array(y)}')\n",
        "\n",
        "\n",
        "\n",
        "array(y).view()\n",
        "array(teste_y).view()\n",
        "\n",
        "\n",
        "model = SVC()\n",
        "neigh = KNeighborsClassifier(n_neighbors=15)\n",
        "\n",
        "#treino SVM\n",
        "model.fit(treino_x,treino_y)\n",
        "prediction = model.predict(teste_x)\n",
        "\n",
        "#previsão SVM\n",
        "prediction.view()\n",
        "accuracy = accuracy_score(teste_y, prediction) * 100\n",
        "\n",
        "#dummy prediction\n",
        "dummy_stratified = DummyClassifier()\n",
        "dummy_stratified.fit(treino_x, treino_y)\n",
        "previsoes = dummy_stratified.predict(teste_x)\n",
        "accuracy_dummy = accuracy_score(teste_y, previsoes) * 100\n",
        "\n",
        "\n",
        "\n",
        "#KNN\n",
        "neigh.fit(treino_x, treino_y)\n",
        "prediction_knn = neigh.predict(teste_x)\n",
        "accuracy_knn = accuracy_score(teste_y, prediction_knn) * 100\n",
        "\n",
        "teste = confusion_matrix(teste_y, prediction)\n",
        "\n",
        "#resultados da execução\n",
        "print(f'Acurácia SVM = {accuracy}')\n",
        "print(f'Acurácia KNN = {accuracy_knn}')\n",
        "print(f'Acurácia dummy = {accuracy_dummy}')\n",
        "results_svm = pd.DataFrame({'Prediction SVM': prediction, 'Prediction KNN': prediction_knn ,'Real value': teste_y})\n",
        "print(results_svm)"
      ],
      "metadata": {
        "id": "OFdCkh1WFmlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Matriz de confusão\n",
        "def matrix_values(result_matrix, emotion):\n",
        "  #iniciação dos valores derivados da matriz (falso negativo, falso positivo, verdadeiro negativo e verdadeiro positivo )\n",
        "  fn = 0\n",
        "  fp = 0\n",
        "  vn = 0\n",
        "  vp = 0\n",
        "  index = np.where(labels == emotion)[0][0]\n",
        "  vp = result_matrix[index,index]\n",
        "  for i in range(len(result_matrix)):\n",
        "    fn += result_matrix[index,i]\n",
        "    fp += result_matrix[i,index]\n",
        "  fn = fn - vp\n",
        "  fp = fp - vp\n",
        "  total = sum(sum(result_matrix))\n",
        "  vn = total - fn - fp - vp\n",
        "\n",
        "  #calculando métricas da matriz de confusão\n",
        "  accuracy_matrix = (vp+vn)/total *100\n",
        "  sensibility = vp/(vp + fn) *100\n",
        "  precision = vp/(vp + fp) *100\n",
        "  specificity = vn/(vn+fp) *100\n",
        "  f1_score = 2*((precision*sensibility)/(precision+sensibility))\n",
        "  print(f'{emotion}')\n",
        "  print(f'vp = {vp}, vn = {vn}, fn = {fn} e fp = {fp}')\n",
        "  print(\"Valores derivados da matriz\")\n",
        "  print(f'Acurácia = {accuracy_matrix:.2f}% \\nSensibilidade = {sensibility:.2f}% \\nPrecisão = {precision:.2f}% \\nEspecificidade = {specificity:.2f}% \\nF1-score = {f1_score:.2f} ')"
      ],
      "metadata": {
        "id": "HUXQ2tIb5lhk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gerando matriz de confusão\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "import seaborn as sns\n",
        "\n",
        "result_matrix = confusion_matrix(teste_y, prediction, normalize='true')\n",
        "labels= unique_labels(teste_y)\n",
        "np.insert(labels, 6, 'SURPRESO')\n",
        "print(labels)\n",
        "colunas = [f'Previsto {label}' for label in labels]\n",
        "indeces = [f'Real {label}' for label in labels]\n",
        "table = pd.DataFrame(result_matrix, \n",
        "                      columns = colunas, index = indeces)\n",
        "sns.heatmap(table, annot= True, fmt = '.1f', cmap='crest' )\n",
        "for emotion in labels:\n",
        "  matrix_values(result_matrix, emotion)\n",
        "\n"
      ],
      "metadata": {
        "id": "mEqP9F2ejOuZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}